{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Demo_Getting_Started_wGCP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngp1f_inOSki"
      },
      "source": [
        "## **TODO:** Set the values of\n",
        "* `PROJECT`\n",
        "* `BUCKET` \n",
        "* `REGION`\n",
        "* `GOOGLE_APPLICATION_CREDENTIALS`\n",
        "\n",
        "below to the values from your Google Cloud Platform environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtKSE83yubQ6"
      },
      "source": [
        "os.environ['PROJECT'] = None # REPLACE WITH YOUR PROJECT ID\n",
        "os.environ['BUCKET'] = None # REPLACE WITH YOUR BUCKET NAME\n",
        "os.environ['REGION'] = None # REPLACE WITH YOUR BUCKET REGION e.g. us-central1\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = None # REPLACE WITH PATH TO YOUR SERVICE ACCOUNT KEY"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTrRw6qZWKQX"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clL2nK9-VX8u"
      },
      "source": [
        "%%bash\n",
        "gcloud auth activate-service-account --key-file $GOOGLE_APPLICATION_CREDENTIALS\n",
        "\n",
        "gcloud config set project $PROJECT\n",
        "gcloud config set compute/region $REGION"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCPAUCV3jal6"
      },
      "source": [
        "!mkdir -p src src/trainer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1l18J9qmudK"
      },
      "source": [
        "%%writefile src/requirements.txt\n",
        "tensorflow>=2.3,<2.4\n",
        "cloudml-hypertune\n",
        "numpy\n",
        "pandas\n",
        "six"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJ1abgqUEvY1"
      },
      "source": [
        "!pip install -r src/requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZ_BfoGTm3RR"
      },
      "source": [
        "!touch src/trainer/__init__.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6wOhCIhn-Tl"
      },
      "source": [
        "%%writefile src/trainer/task.py\n",
        "import os\n",
        "import json\n",
        "import argparse\n",
        "import hypertune\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow.data as tfd\n",
        "from datetime import datetime\n",
        "from tensorflow.data import Dataset\n",
        "\n",
        "# building blocks of our network\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "print(f\"TensorFlow version {tf.__version__}\")\n",
        "\n",
        "def glob_to_ds(glob, num_epochs = 1, batch_size = 1):\n",
        "  def decode(x):\n",
        "    yX = tf.stack(list(x.values()), axis = 1)\n",
        "    return yX[:, 1:], yX[:, 0]\n",
        "  \n",
        "  ds = (tf.data.experimental\n",
        "        .make_csv_dataset(glob, \n",
        "                          num_epochs = num_epochs,\n",
        "                          batch_size = batch_size)\n",
        "        .map(decode))\n",
        "  \n",
        "  return ds\n",
        "\n",
        "def train(hparams):\n",
        "  print(f\"hparams={hparams}\")\n",
        "\n",
        "  JOB_DIR = hparams['JOB_DIR']\n",
        "  BATCH_SIZE = hparams['BATCH_SIZE']\n",
        "  FEATURES = hparams['FEATURES']\n",
        "  BUCKET = hparams['BUCKET']\n",
        "  EPOCHS = hparams['EPOCHS']\n",
        "  LEARNING_RATE = hparams['LEARNING_RATE']\n",
        "\n",
        "  [train_ds, valid_ds, test_ds] = [glob_to_ds(glob, \n",
        "                          batch_size = BATCH_SIZE) for glob in \\\n",
        "                          [f\"gs://{BUCKET}/train/part-*.csv\",\n",
        "                          f\"gs://{BUCKET}/valid/part-*.csv\",\n",
        "                          f\"gs://{BUCKET}/test/part-*.csv\"]]\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Dense(units=13, input_shape=[len(FEATURES)], activation='relu'))\n",
        "  model.add(Dense(units=13, activation='relu'))\n",
        "  model.add(Dense(units=13, activation='relu'))\n",
        "  model.add(Dense(units=1, activation='linear'))\n",
        "\n",
        "  tb_cb = tf.keras.callbacks.TensorBoard(log_dir=f\"{JOB_DIR}/logs\")\n",
        "\n",
        "  class HpoCallback(tf.keras.callbacks.Callback):\n",
        "      def __init__(self):\n",
        "          self.hpt = hypertune.HyperTune()\n",
        "\n",
        "      def on_epoch_end(self, epoch, logs):\n",
        "          print(f\"*** REPORTING val_loss={logs['val_loss']}\")\n",
        "          self.hpt.report_hyperparameter_tuning_metric(\n",
        "              hyperparameter_metric_tag='val_loss',\n",
        "              metric_value=logs['val_loss'],\n",
        "              global_step=epoch\n",
        "          )  \n",
        "  hpo_cb = HpoCallback()\n",
        "\n",
        "  mc_cb = tf.keras.callbacks.ModelCheckpoint(\n",
        "      filepath=JOB_DIR + \"/model-epoch-{epoch:02d}-val-mse-{val_loss:.4f}\",\n",
        "      save_best_only=True, \n",
        "      monitor=\"val_loss\",\n",
        "      verbose=1,\n",
        "  )\n",
        "\n",
        "  model.compile(optimizer = Adam( learning_rate = LEARNING_RATE ), \n",
        "                loss = tf.keras.losses.MSE,  \n",
        "                metrics = [tf.keras.metrics.RootMeanSquaredError(), \n",
        "                        tf.keras.metrics.MeanAbsoluteError()]) \n",
        "\n",
        "  training = model.fit(train_ds, \n",
        "                      validation_data = valid_ds,\n",
        "                      epochs = EPOCHS, \n",
        "                      callbacks=[tb_cb, mc_cb, hpo_cb],\n",
        "                      verbose = 2,)\n",
        "  \n",
        "  results = model.evaluate(test_ds)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  hparams = {\n",
        "      'JOB_ID': 'tmp03',\n",
        "      'FEATURES': ['origin_block_latitude',\n",
        "                   'origin_block_longitude',\n",
        "                   'destination_block_latitude',\n",
        "                   'destination_block_longitude'],\n",
        "      'TARGET': ['fareamount'],\n",
        "      'BUCKET': 'cfai-tmp03',\n",
        "      'BATCH_SIZE': 2 ** 18,\n",
        "      'LEARNING_RATE': 0.003,\n",
        "      'EPOCHS': 1,      \n",
        "  }\n",
        "  hparams = {\n",
        "      'JOB_DIR': hparams['JOB_ID'],\n",
        "      **hparams,\n",
        "  }  \n",
        "\n",
        "  parser = argparse.ArgumentParser()\n",
        "  parser.add_argument('--LEARNING_RATE', type=float)\n",
        "  parser.add_argument('--BATCH_SIZE', type=int)\n",
        "  parser.add_argument('--EPOCHS', type=int)\n",
        "  parser.add_argument('--job_dir','--job-dir', type=str)\n",
        "  args = vars(parser.parse_args())\n",
        "\n",
        "  for k, v in args.items():\n",
        "    hparams[str.upper(k)] = v if v else hparams[str.upper(k)]\n",
        "  print(f\"hparams={hparams}\")\n",
        "\n",
        "  #if started using gcloud ai-platform\n",
        "  if 'TF_CONFIG' in os.environ and \\\n",
        "    'job_dir' in json.loads(os.environ['TF_CONFIG'])['job']:\n",
        "\n",
        "    tf_config = json.loads(os.environ['TF_CONFIG'])\n",
        "    print(json.dumps(tf_config, indent=2))\n",
        "\n",
        "    job_dir = tf_config['job']['job_dir']\n",
        "    print(f\"*** job_dir: {job_dir}\")\n",
        "\n",
        "    task_type = tf_config['task']['type']\n",
        "    print(f\"*** task_type: {task_type}\")\n",
        "\n",
        "    task_index = tf_config['task']['index']\n",
        "    print(f\"*** task_index: {task_index}\")\n",
        "\n",
        "    num_workers = len(tf_config['cluster']['worker']) if 'worker' in tf_config['cluster'] else 1\n",
        "    print(f\"*** num_workers: {num_workers}\")\n",
        "\n",
        "    BATCH_SIZE = hparams['BATCH_SIZE'] // num_workers\n",
        "    print(f\"*** BATCH_SIZE: {BATCH_SIZE}\")\n",
        "\n",
        "    job_id = os.environ['CLOUD_ML_JOB_ID'] \\\n",
        "              if 'CLOUD_ML_JOB_ID' in os.environ else hparams['JOB_ID']\n",
        "    print(f\"*** job_id={job_id}\")\n",
        "\n",
        "    train({\n",
        "        **hparams,\n",
        "        'BATCH_SIZE': BATCH_SIZE,\n",
        "        'JOB_DIR': job_dir,\n",
        "    })\n",
        "\n",
        "  else:\n",
        "    train(hparams)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmavtTK7pyfn"
      },
      "source": [
        "!cd src ; gcloud ai-platform local train \\\n",
        "--package-path trainer \\\n",
        "--module-name trainer.task \\\n",
        "-- \\\n",
        "--BATCH_SIZE=65536 \\\n",
        "--job-dir=tmp04"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07CseVD8sXsr"
      },
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "ts = datetime.now().strftime(\"%Y%m%d%H%M%S%f\")\n",
        "\n",
        "os.environ['JOB_NAME'] = f\"job{ts}\"\n",
        "os.environ['JOB_DIR'] = f\"gs://{os.environ['BUCKET']}/{os.environ['JOB_NAME']}/keras-job-dir\"\n",
        "print(os.environ['JOB_NAME'], os.environ['JOB_DIR'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFfK_bkYLZyx"
      },
      "source": [
        "!cd src ; gcloud ai-platform jobs submit training $JOB_NAME \\\n",
        "--package-path trainer/ \\\n",
        "--module-name trainer.task \\\n",
        "--region $REGION \\\n",
        "--python-version 3.7 \\\n",
        "--runtime-version 2.4 \\\n",
        "--job-dir $JOB_DIR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U89-IoFtHbcX"
      },
      "source": [
        "%%writefile src/hpconfig.yaml\n",
        "trainingInput:\n",
        "  hyperparameters:\n",
        "    goal: MINIMIZE\n",
        "    hyperparameterMetricTag: \"val_loss\"\n",
        "    maxTrials: 4\n",
        "    maxParallelTrials: 2\n",
        "    params:\n",
        "      - parameterName: LEARNING_RATE\n",
        "        type: DOUBLE\n",
        "        minValue: 0.003\n",
        "        maxValue: 0.3\n",
        "        scaleType: UNIT_LOG_SCALE\n",
        "      - parameterName: EPOCHS\n",
        "        type: INTEGER\n",
        "        minValue: 1\n",
        "        maxValue: 10\n",
        "        scaleType: UNIT_LINEAR_SCALE\n",
        "      - parameterName: BATCH_SIZE\n",
        "        type: DISCRETE\n",
        "        discreteValues: [8192, 65536, 262144]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmEwldzXp2jl"
      },
      "source": [
        "!cd src ; gcloud ai-platform jobs submit training $JOB_NAME \\\n",
        "--config hpconfig.yaml \\\n",
        "--package-path trainer/ \\\n",
        "--module-name trainer.task \\\n",
        "--region $REGION \\\n",
        "--python-version 3.7 \\\n",
        "--runtime-version 2.3 \\\n",
        "--job-dir $JOB_DIR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1id3XrhCjBbG"
      },
      "source": [
        "!gcloud ai-platform models create $JOB_NAME --region $REGION"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yu1V0QRokd5z"
      },
      "source": [
        "MODEL_LOCATION = !gsutil ls -d $JOB_DIR/model* | head -n 1\n",
        "[MODEL_LOCATION] = MODEL_LOCATION\n",
        "os.environ['MODEL_LOCATION'] = MODEL_LOCATION"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiiiNpoikMAk"
      },
      "source": [
        "!gcloud ai-platform versions create latest \\\n",
        "--model $JOB_NAME \\\n",
        "--origin $MODEL_LOCATION \\\n",
        "--region $REGION \\\n",
        "--framework tensorflow \\\n",
        "--runtime-version 2.3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VdoTVW2o4OQ"
      },
      "source": [
        "%%bash\n",
        "gcloud ai-platform predict \\\n",
        "--model $JOB_NAME \\\n",
        "--json-instances test.json \\\n",
        "--region $REGION \\\n",
        "--version latest"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}